# Алгоритмы анализа данных
### Урок 1. Алгоритм линейной регрессии. Градиентный спуск

- Подберите скорость обучения (alpha) и количество итераций
- В этом коде мы избавляемся от итераций по весам, но здесь есть ошибка, исправьте её. (код в материалах к уроку)
- Вместо того чтобы задавать количество итераций, задайте условие остановки алгоритма, когда ошибка за итерацию начинает изменяться ниже определённого порога — упрощённый аналог параметра tol в линейной регрессии в sklearn.

---

### Урок 2. Масштабирование признаков. L1- и L2-регуляризация. Стохастический градиентный спуск
- Постройте график зависимости весов всех признаков от lambda в L2-регуляризации на основе данных из урока.
- Можно ли к одному и тому же признаку применить сразу и нормализацию, и стандартизацию?
- Напишите функцию наподобие eval_model_reg2, но для применения L1-регуляризации.

---

### Урок 3. Логистическая регрессия. Log Loss
- Измените функцию calc_logloss так, чтобы нули по возможности не попадали в np.log.
- Подберите аргументы функции eval_model для логистической регрессии таким образом, чтобы log loss был минимальным.
- Создайте функцию calc_pred_proba, возвращающую предсказанную вероятность класса 1. На вход подаётся W, который уже посчитан функцией eval_model, и X, на выходе — массив y_pred_proba.
- Создайте функцию calc_pred, возвращающую предсказанный класс. На вход подаётся W, который уже посчитан функцией eval_model, и X, на выходе — массив y_pred.
- Посчитайте Accuracy, матрицу ошибок, точность и полноту, а также F1 score.
- Могла ли модель переобучиться? Почему?